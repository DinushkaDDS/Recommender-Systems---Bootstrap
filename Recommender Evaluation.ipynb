{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "># Evaluation and Testing of Recommendations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Typical machine learning metrics cant be used in recommender systems due to their nature. Therefore in order to understand the performance of the recommendations (whether recommendations are good) we need to have different methodologies. Usually live system is the best way to understand this. But we need pointers to figure out the genaral direction of the recommendations during development as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One such method is called `coverage`. This answers whether all the users get a recommendation and does all the items get recommended. ie:- user coverage and content coverage.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def coverage(users, items, recommender):\r\n",
    "    users_with_recommendations = 0\r\n",
    "    recommended_items = set()\r\n",
    "\r\n",
    "    for user in users:\r\n",
    "        recommendations_for_user = recommender.recommend(user.id, items)\r\n",
    "        if ((recommendations_for_user != None) and len(recommendations_for_user) >0):\r\n",
    "            users_with_recommendations += 1\r\n",
    "\r\n",
    "            for recommended_item in recommendations_for_user:\r\n",
    "                recommended_items.add(recommended_item)\r\n",
    "    \r\n",
    "    total_users = len(users)\r\n",
    "    total_items = len(items)\r\n",
    "\r\n",
    "    user_converage = users_with_recommendations/total_users\r\n",
    "    item_coverage = len(recommended_items)/total_items\r\n",
    "\r\n",
    "    return user_converage, item_coverage"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">It is important to have tests for the smaller pieces of the code itself.\r\n",
    "\r\n",
    "eg:- In similarity checks we can look whether the same vector similarity returns 1. orthogonal vectors return 0 and opposite vectors return -1 etc.\r\n",
    "\r\n",
    "To do further testing, we need to have a test dataset called `complete ground truth`. This should contain all the user and item combinations. In practice we would never have something like this because it makes the recommendation systems irrelavant. But for testing purposes we can create a synthetic one and progress upon it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Offline testing\r\n",
    "\r\n",
    "The simplest (least useful) way of testing a recommender system is taking the historical data(ground truth) and then cover part of the ratings given by users to items. Then we can use the recommender system to rate those covered ratings. Once it is done, we can compare the recommender outputs and actual rating values using MSE, RMSE etc.\r\n",
    "\r\n",
    "This type of evalution does not work well in practise (apparently)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Desicion Support Matrices\r\n",
    "\r\n",
    "In typical ML project we have classification evaluation metrics such as precision, recall and accuracy. But in recommendation systems, identifing the related values for calculation of above metrics are bit complicated. To do that we define below values tailored to a recommendation system.\r\n",
    "\r\n",
    "- True Positive: Item recommended and Consumed by the User.\r\n",
    "- False Positive: Item was recommended but the user did not consume it.\r\n",
    "- False Negative: The recommender did not recommend, but user consumed.\r\n",
    "- True Negative: Recommender did recommend, user did not consume as well.\r\n",
    "\r\n",
    "\r\n",
    "<center><image src=\"./images/Evaluation Metrics.jpg\" width=\"500px\" /></center>\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Mean Average Precision**\r\n",
    "\r\n",
    "The average precision can be measured to see how good a rank is by taking the precision for top k recommended items. Here m indicated recommended items while k denotes the relevant items. (Per user recommendation)\r\n",
    "\r\n",
    "<center><image src=\"./images/Precision for k.jpg\" width=\"350px\" /></center>\r\n",
    "\r\n",
    "<center><image src=\"./images/Mean average precision.jpg\" width=\"250px\" /></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Discounted Cumulative Gain**\r\n",
    "\r\n",
    "In this metric we not only consider the relevancy of an item, we also consider the position(ranking) of that item to the calculation. In the below equation rel[i] can be the predicted rating, business profit for the item etc. depending on the use case.\r\n",
    "\r\n",
    "<center><image src=\"./images/Discounted cumulative gain.jpg\" width=\"250px\" /></center>\r\n",
    "\r\n",
    "For more details of the implementation check [This Link](https://github.com/benhamner/Metrics)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Online Evaluation\r\n",
    "\r\n",
    "2 main methods of online testing are as follows.\r\n",
    "\r\n",
    "1. Controlled Experiment: In this method we expose the recommendation system to closed group and get feedback from them.\r\n",
    "2. A/B Testing: Basically we divert a part of users to new recommendations while others were given with usual implementation. This is useful to understand the impact of the recommendation systems compared to what already exist.\r\n",
    "\r\n",
    "Either way the point is to identify the parameters/techniques that would provide better recommendations to users."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also one important concept in recommendation systems is `Feedback Loops`. In this concept it is said that when we do recommendations, users may get stuck in same type of items as the system progress. In order to keep the diversity of content recommended to users it is essential to add new items as inputs. Those can be random items, searches done by users etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}