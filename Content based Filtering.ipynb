{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "># Content based filtering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared to collabarative filtering, the difference of content based filtering is that this uses the features of a given item for recommendation. Basically in this type of recommendation systems, normal ML classification and similarity techniques will be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the implementation of such system, there are 3 main parts that we need to consider.\r\n",
    "\r\n",
    "1. Content analyzer - Basically offline training of a model on the items. We may build vectors/profiles for each item which can be used later during recommendation process.\r\n",
    "\r\n",
    "2. User Profiler -  We create profiles for users as well. This is to identify the unique person interests to match against the items of our system.\r\n",
    "\r\n",
    "3. Item Retriever - The inference process where we match the user profile with items to get the recommendation. This is an online process.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Content Analyzer\r\n",
    "\r\n",
    "Mainly based on what we called metadata. This include tags, descriptions, reviews etc. about an item. One of the important yet annoying part of this is recognizing what to use and what not to use. Because adding all will be expensive(computationally/development wise and actual cost) and adding less/ wrong features will yield in bad/weird recommendations.\r\n",
    "\r\n",
    "This closely relates to NLP. Therefore many techniques in NLP world can e used such as Bag of Words, word2vec, TF-IDF, transformers, data cleaning steps(lematization, stop word removal). Some interesting pointers to related techniques are below.\r\n",
    "\r\n",
    "\r\n",
    "- *Theres a python package named `stop-words` which include stop words from various languages. Install it using `pip install stop-words`.*\r\n",
    "\r\n",
    "- *We can remove highest occuring and lowest occuring tokens from the dataset, if we are using a token based string vectorization method to transform strings.*\r\n",
    "\r\n",
    "- *Can use stemmer/lemmatizer to reduce token forms in dataset as well. But again usefulness may depend on the application and the data you have.*\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### TF-IDF\r\n",
    "\r\n",
    "- TF = Term Frequency (how many times the word appear in the document.)\r\n",
    "- IDF = Inverse Document Frequency (Measure of how many documents have the considering word. If few documents have the considering word, higher the value would be.)\r\n",
    "\r\n",
    "We take the log forms of above measures for calculations.\r\n",
    "\r\n",
    "__<center>TF-IDF = TF(word, document)*IDF(word, documents)</center>__\r\n",
    "\r\n",
    "<center>TF(word, document) = 1 + log(word frequency in the document)</center>\r\n",
    "<center>IDF(word, documents) = log(Total number of documents) - log(num of documents with considering word)</center>\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### LDA (Latent Dirichlet Allocation)\r\n",
    "\r\n",
    "In this ML technique words get allocated to hidden(latent) topics in the document distribution. Then those topics will be used to describe a document in a mathematical formula of percentages.\r\n",
    "\r\n",
    "\r\n",
    "<center><image src=\"./images/LDA training.jpg\" width=\"500px\" /></center>\r\n",
    "\r\n",
    "* Can use `pyLDAvis` package to visualize the LDA topics distribution. This helps to understand the proper topic number \"k\" we need to use for the algorithm.\r\n",
    "* There are parameters `alpha` and `beta` in LDA which can be used to finetune the topic and word dstributions.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF IDF Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "from collections import Counter, defaultdict\r\n",
    "import math\r\n",
    "\r\n",
    "def tf_idf(documents):\r\n",
    "\r\n",
    "    document_tfs = []\r\n",
    "    token_counter = defaultdict(lambda:0)\r\n",
    "\r\n",
    "    for document in documents:\r\n",
    "        tokens = document.split(\" \")\r\n",
    "        counts = Counter(tokens)\r\n",
    "\r\n",
    "        for token in set(tokens):\r\n",
    "            token_counter[token] += 1\r\n",
    "        document_tfs.append(counts)\r\n",
    "\r\n",
    "\r\n",
    "    base_vector = list(token_counter.keys())    \r\n",
    "    document_vectors = []\r\n",
    "    for doc in document_tfs:\r\n",
    "        vec = []\r\n",
    "        for key in base_vector:\r\n",
    "            tf = (doc[key]/sum(doc.values()))\r\n",
    "            idf = math.log(len(documents)/(token_counter[key]))\r\n",
    "            vec.append(tf*idf)\r\n",
    "        document_vectors.append(vec)\r\n",
    "\r\n",
    "    return document_vectors\r\n",
    "\r\n",
    "\r\n",
    "tf_idf([\"my name is dilan\", \"my name is dinushka\", \"I suck at statistics.\", \"I love my dog\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0.34657359027997264,\n",
       "  0.17328679513998632,\n",
       "  0.17328679513998632,\n",
       "  0.07192051811294521,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.17328679513998632,\n",
       "  0.17328679513998632,\n",
       "  0.07192051811294521,\n",
       "  0.34657359027997264,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.34657359027997264,\n",
       "  0.34657359027997264,\n",
       "  0.17328679513998632,\n",
       "  0.34657359027997264,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07192051811294521,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17328679513998632,\n",
       "  0.0,\n",
       "  0.34657359027997264,\n",
       "  0.34657359027997264]]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### TF IDF is meant to be used in applications where we know the inputs beforehand. As you can see we need to have the total counts in all the documents to get the IDF value. Also once we add a new document with overlapping tokens with other documents, it causes their idf values to change."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA Implementation\r\n",
    "\r\n",
    "Based on the following 2 youtube videos regarging the theory of LDA and Gibbs Sampling.\r\n",
    "\r\n",
    "1. [First Video](https://www.youtube.com/watch?v=T05t-SqKArY)\r\n",
    "2. [Second Video](https://www.youtube.com/watch?v=BaM1uiCpj_E) \r\n",
    "\r\n",
    "[Gibbs Sampling Explanation](https://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html#latent-dirichlet-allocation)  --> This is very good article/implementation details on Gibbs sampling for LDA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "documents =[ \"my name is dilan\", \r\n",
    "             \"my name is dinushka\", \r\n",
    "             \"I suck at statistics.\", \r\n",
    "             \"I love my dog very much\"]\r\n",
    "\r\n",
    "tokenized_documents = [doc.split(\" \") for doc in documents]\r\n",
    "vocabulary = set()\r\n",
    "token2index = {}\r\n",
    "i = 0\r\n",
    "for doc in tokenized_documents:\r\n",
    "    for token in doc:\r\n",
    "        vocabulary.add(token)\r\n",
    "        if(token not in token2index.keys()):\r\n",
    "            token2index[token] = i\r\n",
    "            i += 1\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from random import choice, choices\r\n",
    "    \r\n",
    "\r\n",
    "k = 3 # num of topics\r\n",
    "\r\n",
    "\r\n",
    "word_topic = np.zeros((k, len(vocabulary)))\r\n",
    "document_topic = np.zeros((len(tokenized_documents), k))\r\n",
    "\r\n",
    "\r\n",
    "# Randomly assign a topic to word in the document and\r\n",
    "# updating the related counts in word_topic and document_topic matrices\r\n",
    "topic_assignments = []\r\n",
    "for doc in range(len(tokenized_documents)):\r\n",
    "    topics = []\r\n",
    "    for idx in range(len(tokenized_documents[doc])):\r\n",
    "        topic_index = choice(range(k))\r\n",
    "\r\n",
    "        token = tokenized_documents[doc][idx]\r\n",
    "        token_index = token2index[token]\r\n",
    "\r\n",
    "        topics.append(topic_index)\r\n",
    "        word_topic[topic_index][token_index] += 1\r\n",
    "        document_topic[doc][topic_index] += 1\r\n",
    "\r\n",
    "    topic_assignments.append(topics)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(topic_assignments)\r\n",
    "print(\"-------------------------------\")\r\n",
    "print(token2index.keys())\r\n",
    "print(\"-------------------------------\")\r\n",
    "print(word_topic)\r\n",
    "print(\"-------------------------------\")\r\n",
    "print(document_topic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1, 0, 2, 1], [1, 2, 0, 1], [0, 2, 1, 1], [0, 1, 1, 2, 2, 0]]\n",
      "-------------------------------\n",
      "dict_keys(['my', 'name', 'is', 'dilan', 'dinushka', 'I', 'suck', 'at', 'statistics.', 'love', 'dog', 'very', 'much'])\n",
      "-------------------------------\n",
      "[[0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [3. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]]\n",
      "-------------------------------\n",
      "[[1. 2. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 2. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><image src=\"./images/LDA Gibbs 1.jpg\" width=\"650px\" /></center>\r\n",
    "<center><image src=\"./images/LDA Gibbs 2.jpg\" width=\"500px\" /></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "from copy import deepcopy\r\n",
    "\r\n",
    "alpha = 1\r\n",
    "beta = 1\r\n",
    "\r\n",
    "for doc_idx, token_lst in enumerate(topic_assignments):\r\n",
    "    for token_idx, topic in enumerate(token_lst):\r\n",
    "        token = tokenized_documents[doc_idx][token_idx]\r\n",
    "        vocab_index = token2index[token]\r\n",
    "        \r\n",
    "        Cwt_Wij = deepcopy(word_topic[:, vocab_index])\r\n",
    "        Cwt_Wij[topic] = Cwt_Wij[topic] - 1\r\n",
    "\r\n",
    "        Sum_Cwt_Wij = np.sum(word_topic, axis=1)\r\n",
    "        Sum_Cwt_Wij[topic] = Sum_Cwt_Wij[topic] - 1\r\n",
    "\r\n",
    "        left = (Cwt_Wij + alpha)/(Sum_Cwt_Wij + (len(vocabulary)*beta))\r\n",
    "\r\n",
    "        Cdt_Dij = deepcopy(document_topic[doc_idx])\r\n",
    "        Cdt_Dij[topic] = Cdt_Dij[topic] - 1\r\n",
    "\r\n",
    "        Sum_Cdt_Dij = np.sum(document_topic[doc_idx]) - 1\r\n",
    "        right = (Cdt_Dij + alpha)/(Sum_Cdt_Dij + (k*alpha))\r\n",
    "\r\n",
    "        prob_dist = (left*right)/sum(left*right)\r\n",
    "\r\n",
    "        new_topic = np.random.choice(range(k), 1, p=prob_dist)\r\n",
    "        print(topic, new_topic)\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "# Just remember after drawing the new topic we also have to update the\r\n",
    "# topic assignment list with newly sampled topic for token w; re-increment the word-topic \r\n",
    "# and document-topic count matrices with the new sampled topic for token w.\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 [0]\n",
      "0 [1]\n",
      "2 [0]\n",
      "1 [2]\n",
      "1 [1]\n",
      "2 [0]\n",
      "0 [2]\n",
      "1 [0]\n",
      "0 [2]\n",
      "2 [1]\n",
      "1 [2]\n",
      "1 [0]\n",
      "0 [2]\n",
      "1 [2]\n",
      "1 [2]\n",
      "2 [0]\n",
      "2 [2]\n",
      "0 [2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Pros & Cons of Content based Filtering\r\n",
    "\r\n",
    "- New items are easy to add, calculate the item vector and we are good to go.\r\n",
    "- Dont need historical data. Just need to have details about the item.\r\n",
    "- Not related to popularity, so wider recommendations.\r\n",
    "- Limited understanding about items may give bad results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}