{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "># Content based filtering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared to collabarative filtering, the difference of content based filtering is that this uses the features of a given item for recommendation. Basically in this type of recommendation systems, normal ML classification and similarity techniques will be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the implementation of such system, there are 3 main parts that we need to consider.\r\n",
    "\r\n",
    "1. Content analyzer - Basically offline training of a model on the items. We may build vectors/profiles for each item which can be used later during recommendation process.\r\n",
    "\r\n",
    "2. User Profiler -  We create profiles for users as well. This is to identify the unique person interests to match against the items of our system.\r\n",
    "\r\n",
    "3. Item Retriever - The inference process where we match the user profile with items to get the recommendation. This is an online process.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Content Analyzer\r\n",
    "\r\n",
    "Mainly based on what we called metadata. This include tags, descriptions, reviews etc. about an item. One of the important yet annoying part of this is recognizing what to use and what not to use. Because adding all will be expensive(computationally/development wise and actual cost) and adding less/ wrong features will yield in bad/weird recommendations.\r\n",
    "\r\n",
    "This closely relates to NLP. Therefore many techniques in NLP world can e used such as Bag of Words, word2vec, TF-IDF, transformers, data cleaning steps(lematization, stop word removal). Some interesting pointers to related techniques are below.\r\n",
    "\r\n",
    "\r\n",
    "- *Theres a python package named `stop-words` which include stop words from various languages. Install it using `pip install stop-words`.*\r\n",
    "\r\n",
    "- *We can remove highest occuring and lowest occuring tokens from the dataset, if we are using a token based string vectorization method to transform strings.*\r\n",
    "\r\n",
    "- *Can use stemmer/lemmatizer to reduce token forms in dataset as well. But again usefulness may depend on the application and the data you have.*\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### TF-IDF\r\n",
    "\r\n",
    "- TF = Term Frequency (how many times the word appear in the document.)\r\n",
    "- IDF = Inverse Document Frequency (Measure of how many documents have the considering word. If few documents have the considering word, higher the value would be.)\r\n",
    "\r\n",
    "We take the log forms of above measures for calculations.\r\n",
    "\r\n",
    "__<center>TF-IDF = TF(word, document)*IDF(word, documents)</center>__\r\n",
    "\r\n",
    "<center>TF(word, document) = 1 + log(word frequency in the document)</center>\r\n",
    "<center>IDF(word, documents) = log(Total number of documents) - log(num of documents with considering word)</center>\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### LDA (Latent Dirichlet Allocation)\r\n",
    "\r\n",
    "In this ML technique words get allocated to hidden(latent) topics in the document distribution. Then those topics will be used to describe a document in a mathematical formula of percentages.\r\n",
    "\r\n",
    "\r\n",
    "<center><image src=\"./images/LDA training.jpg\" width=\"500px\" /></center>\r\n",
    "\r\n",
    "* Can use `pyLDAvis` package to visualize the LDA topics distribution. This helps to understand the proper topic number \"k\" we need to use for the algorithm.\r\n",
    "* There are parameters `alpha` and `beta` in LDA which can be used to finetune the topic and word dstributions.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF IDF Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "from collections import Counter, defaultdict\r\n",
    "import math\r\n",
    "\r\n",
    "def tf_idf(documents):\r\n",
    "\r\n",
    "    document_tfs = []\r\n",
    "    token_counter = defaultdict(lambda:0)\r\n",
    "\r\n",
    "    for document in documents:\r\n",
    "        tokens = document.split(\" \")\r\n",
    "        counts = Counter(tokens)\r\n",
    "\r\n",
    "        for token in set(tokens):\r\n",
    "            token_counter[token] += 1\r\n",
    "        document_tfs.append(counts)\r\n",
    "\r\n",
    "\r\n",
    "    base_vector = list(token_counter.keys())    \r\n",
    "    document_vectors = []\r\n",
    "    for doc in document_tfs:\r\n",
    "        vec = []\r\n",
    "        for key in base_vector:\r\n",
    "            tf = (doc[key]/sum(doc.values()))\r\n",
    "            idf = math.log(len(documents)/(token_counter[key]))\r\n",
    "            vec.append(tf*idf)\r\n",
    "        document_vectors.append(vec)\r\n",
    "\r\n",
    "    return document_vectors\r\n",
    "\r\n",
    "\r\n",
    "tf_idf([\"my name is dilan\", \"my name is dinushka\", \"I suck at statistics.\", \"I love my dog\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0.34657359027997264,\n",
       "  0.17328679513998632,\n",
       "  0.17328679513998632,\n",
       "  0.07192051811294521,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.17328679513998632,\n",
       "  0.17328679513998632,\n",
       "  0.07192051811294521,\n",
       "  0.34657359027997264,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.34657359027997264,\n",
       "  0.34657359027997264,\n",
       "  0.17328679513998632,\n",
       "  0.34657359027997264,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07192051811294521,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17328679513998632,\n",
       "  0.0,\n",
       "  0.34657359027997264,\n",
       "  0.34657359027997264]]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">### TF IDF is meant to be used in applications where we know the inputs beforehand. As you can see we need to have the total counts in all the documents to get the IDF value. Also once we add a new document with overlapping tokens with other documents, it causes their idf values to change."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA Implementation\r\n",
    "\r\n",
    "Based on the following 2 youtube videos regarging the theory of LDA and Gibbs Sampling.\r\n",
    "\r\n",
    "1. [First Video](https://www.youtube.com/watch?v=T05t-SqKArY)\r\n",
    "2. [Second Video](https://www.youtube.com/watch?v=BaM1uiCpj_E) \r\n",
    "\r\n",
    "[Gibbs Sampling Explanation](https://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html#latent-dirichlet-allocation)  --> This is very good article/implementation details on Gibbs sampling for LDA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "def build_vocab(documents):\r\n",
    "    tokenized_documents = [doc.split(\" \") for doc in documents]\r\n",
    "    vocabulary = set()\r\n",
    "    token2index = {}\r\n",
    "    i = 0\r\n",
    "    for doc in tokenized_documents:\r\n",
    "        for token in doc:\r\n",
    "            vocabulary.add(token)\r\n",
    "            if(token not in token2index.keys()):\r\n",
    "                token2index[token] = i\r\n",
    "                i += 1\r\n",
    "    return tokenized_documents, vocabulary, token2index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "def random_topic_assignment(tokenized_documents, k, token2index, word_topic, document_topic):\r\n",
    "    from random import choice\r\n",
    "\r\n",
    "    topic_assignments = []\r\n",
    "    for doc in range(len(tokenized_documents)):\r\n",
    "        topics = []\r\n",
    "        for idx in range(len(tokenized_documents[doc])):\r\n",
    "            topic_index = choice(range(k))\r\n",
    "\r\n",
    "            token = tokenized_documents[doc][idx]\r\n",
    "            token_index = token2index[token]\r\n",
    "\r\n",
    "            topics.append(topic_index)\r\n",
    "            word_topic[topic_index][token_index] += 1\r\n",
    "            document_topic[doc][topic_index] += 1\r\n",
    "\r\n",
    "        topic_assignments.append(topics)\r\n",
    "\r\n",
    "    return topic_assignments"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "def LDA_Training(documents, k=3, iterations = 1000, alpha=1, beta=1):\r\n",
    "    import numpy as np\r\n",
    "    \r\n",
    "    tokenized_documents, vocabulary, token2index = build_vocab(documents)\r\n",
    "\r\n",
    "    # Building Initial matrices\r\n",
    "    word_topic = np.zeros((k, len(vocabulary)))\r\n",
    "    document_topic = np.zeros((len(tokenized_documents), k))\r\n",
    "\r\n",
    "\r\n",
    "    # Randomly assign a topic to word in the document for the initialization of LDA\r\n",
    "    topic_assignments = random_topic_assignment(tokenized_documents, k,\r\n",
    "                                                token2index, word_topic, document_topic)\r\n",
    "\r\n",
    "    # Gibbs sampling for LDA training (Related maths parts are below.)\r\n",
    "    for iter in range(iterations):\r\n",
    "        for doc_idx, token_lst in enumerate(topic_assignments):\r\n",
    "            for token_idx, topic in enumerate(token_lst):\r\n",
    "                token = tokenized_documents[doc_idx][token_idx]\r\n",
    "                vocab_index = token2index[token]\r\n",
    "                \r\n",
    "                Cwt_Wij = word_topic[:, vocab_index]\r\n",
    "                Cwt_Wij[topic] = Cwt_Wij[topic] - 1\r\n",
    "\r\n",
    "                Sum_Cwt_Wij = np.sum(word_topic, axis=1)\r\n",
    "                Sum_Cwt_Wij[topic] = Sum_Cwt_Wij[topic] - 1\r\n",
    "\r\n",
    "                left = (Cwt_Wij + beta)/(Sum_Cwt_Wij + (len(vocabulary)*beta))\r\n",
    "\r\n",
    "                Cdt_Dij = document_topic[doc_idx]\r\n",
    "                Cdt_Dij[topic] = Cdt_Dij[topic] - 1\r\n",
    "\r\n",
    "                Sum_Cdt_Dij = np.sum(document_topic[doc_idx]) - 1\r\n",
    "                right = (Cdt_Dij + alpha)/(Sum_Cdt_Dij + (k*alpha))\r\n",
    "\r\n",
    "                prob_dist = (left*right)/sum(left*right)\r\n",
    "\r\n",
    "                new_topic = np.random.choice(range(k), 1, p=prob_dist)\r\n",
    "\r\n",
    "                # Reupdating the matrices with the new sampled topic\r\n",
    "                topic_assignments[doc_idx][token_idx] = new_topic[0]\r\n",
    "                word_topic[:, vocab_index][new_topic] += 1\r\n",
    "                document_topic[doc_idx][new_topic] += 1\r\n",
    "\r\n",
    "    phi = (word_topic + beta)/(np.reshape(np.sum(word_topic, axis=1), (k,1)) + (len(vocabulary)*beta))\r\n",
    "    theta = (document_topic + alpha)/(np.reshape(np.sum(document_topic, axis=1), (len(documents),1)) + (k*alpha))\r\n",
    "\r\n",
    "    class LDA(object):\r\n",
    "        pass\r\n",
    "    \r\n",
    "    lda = LDA()\r\n",
    "    lda.word_topic = word_topic\r\n",
    "    lda.document_topic = document_topic\r\n",
    "    lda.topic_assignments = topic_assignments\r\n",
    "    lda.vocabulary = token2index\r\n",
    "    lda.word_topic_distibution = np.squeeze(phi)\r\n",
    "    lda.document_topic_distribution = np.squeeze(theta)\r\n",
    "    lda.k = k\r\n",
    "    \r\n",
    "    return lda"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "def get_details(model, documents, doc_id, related_word_count=3):\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    topic_distribution = model.document_topic_distribution[doc_id]\r\n",
    "    print(\"Document\", documents[doc_id])\r\n",
    "    print(\"Document topic distribution: \", topic_distribution)\r\n",
    "\r\n",
    "    idx2token = res = dict((v,k) for k,v in model.vocabulary.items())\r\n",
    "\r\n",
    "    related_words_distribution = model.word_topic_distibution[np.argmax(topic_distribution)]\r\n",
    "    words = []\r\n",
    "    for i in np.argsort(related_words_distribution)[-related_word_count:]:\r\n",
    "        words.append(idx2token[i])\r\n",
    "\r\n",
    "    print(\"Top words for the dominent topics:\", words)\r\n",
    "\r\n",
    "    return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "def cluster_documents(model, documents):\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    clusters = [[] for _ in range(model.k)]\r\n",
    "    for i in range(len(documents)):\r\n",
    "        topic_distribution = model.document_topic_distribution[i]\r\n",
    "        topic = np.argmax(topic_distribution)\r\n",
    "        clusters[topic].append(documents[i])\r\n",
    "\r\n",
    "    for idx, clst in enumerate(clusters):\r\n",
    "        print(\"Cluster\", idx)\r\n",
    "        print(clst)\r\n",
    "        print()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "documents =[ \"my name is dilan\", \r\n",
    "             \"my name is dinushka\",\r\n",
    "             \"my name is senarath\",\r\n",
    "             \"my name is Sheero\",\r\n",
    "             \"I love my dog very much\",\r\n",
    "             \"my dog eat meat\",\r\n",
    "             \"my dog loves runnning.\", \r\n",
    "             \"my dog love to eat meat\",\r\n",
    "             \"statisics is a great subject\",\r\n",
    "             \"statistics is very hard\",\r\n",
    "             \"I love to do statistics\",\r\n",
    "             \"i suck at statistics\"\r\n",
    "             ]\r\n",
    "\r\n",
    "model = LDA_Training(documents, k=3, alpha=0.01, beta=0.1)\r\n",
    "\r\n",
    "cluster_documents(model, documents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cluster 0\n",
      "['my name is dilan', 'my name is dinushka', 'my name is senarath', 'my name is Sheero', 'statisics is a great subject']\n",
      "\n",
      "Cluster 1\n",
      "['statistics is very hard', 'I love to do statistics', 'i suck at statistics']\n",
      "\n",
      "Cluster 2\n",
      "['I love my dog very much', 'my dog eat meat', 'my dog loves runnning.', 'my dog love to eat meat']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "get_details(model, documents, 9)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document statistics is very hard\n",
      "Document topic distribution:  [0.00248139 0.99503722 0.00248139]\n",
      "Top words for the dominent topics: ['is', 'at', 'statistics']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see LDA modelling can be used to clustering data and further analysis on probablities will help to get an understanding about the clusters as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the Gibbs sampling for LDA equations that have been used in the above function. Note that some values are not self explanatory, so please check with the code to see. May be wrong from my end as well. ;()\r\n",
    "\r\n",
    "<center><image src=\"./images/LDA Gibbs 1.jpg\" width=\"650px\" /></center>\r\n",
    "<center><image src=\"./images/LDA Gibbs 2.jpg\" width=\"500px\" /></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">## Pros & Cons of Content based Filtering\r\n",
    "\r\n",
    "- New items are easy to add, calculate the item vector and we are good to go.\r\n",
    "- Dont need historical data. Just need to have details about the item.\r\n",
    "- Not related to popularity, so wider recommendations.\r\n",
    "- Limited understanding about items may give bad results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}