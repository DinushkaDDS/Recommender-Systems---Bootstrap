{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Matrix factorization for finding Hidden genres**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this, we need to look into the rating matrix again. But now comes an additional problem. The more data we have, bigger the matrix become. And therefore computations become time consuming. In order to maintain the data features while minimizing the time consumption(computation power required), we use a technique called `Dimensional reduction`. In this notebook to separate the rating matrix into smaller parts we use a mathematical technique called `Singular Value Decomposition` (SVD)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before going in to the mentioned topics, it is important to understand the basics of matrix factorization. Assume you have a matrix `R` and then we can decompose it in following form.\r\n",
    "\r\n",
    "<center>\r\n",
    "\r\n",
    "__R = U.V__\r\n",
    "</center>\r\n",
    "\r\n",
    "If R has dimensions n\\*m then U will have n\\*d and V will have d\\*m dimensions. This is called `UV-Decomposition`. In recommender systems field, U is the user-feature matrix, V is the item-feature matrix and R will be the ratings matrix. The idea behind the factorization is to find values to U and V which yield their multiplication to R matrix as close as possible, in theory it is basically solving several linear equations. (ie. which satisfy the matrix multiplication.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVD (Singular Value Decomposition)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the most common way of factorizing matrices is SVD. In SVD we construct 3 matrices namely `U`, `V* (V Transpose)` and `Σ (Sigma)`. Here U and V acts as factors while sigma acts as the regulator for the data dimensions.\r\n",
    "<pre style='color:yellow'>\r\n",
    "<center>M = U.Σ.V*</center>\r\n",
    "\r\n",
    "- M  = Data Marix\r\n",
    "- U  = User feature matrix\r\n",
    "- Σ  = Weights diagonal matrix (eigen values matrix)\r\n",
    "- V* = Item Feature matrix\r\n",
    "</pre>\r\n",
    "To get more theoritical understanding behind SVD you can watch [videos in here.](https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&index=1)\r\n",
    "\r\n",
    "Instead of implementing myself, I have used the numpy implementation of the algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "movies = ['mib', 'st', 'av', 'b', 'ss', 'lm']\r\n",
    "users = ['Sara', 'Jesper', 'Therese', 'Helle', 'Pietro', 'Ekaterina']\r\n",
    "M = pd.DataFrame([\r\n",
    "                    [5.0, 3.0, 0.0, 2.0, 2.0, 2.0],\r\n",
    "                    [4.0, 3.0, 4.0, 0.0, 3.0, 3.0],\r\n",
    "                    [5.0, 2.0, 5.0, 2.0, 1.0, 1.0],\r\n",
    "                    [3.0, 5.0, 3.0, 0.0, 1.0, 1.0],\r\n",
    "                    [3.0, 3.0, 3.0, 2.0, 4.0, 5.0],\r\n",
    "                    [2.0, 3.0, 2.0, 3.0, 5.0, 5.0]],\r\n",
    "                columns=movies,\r\n",
    "                index=users)\r\n",
    "\r\n",
    "from numpy import linalg\r\n",
    "\r\n",
    "U, sigma, V_t = linalg.svd(M)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we check the output from above matrices, we will see that their multiplication does not add up to the original values exactly. But close enough to be usable. Also in the sigma matrix (eigen values sorted) we can check the amount of information given by the each of the data rows/columns in the U/V matrices.\r\n",
    "\r\n",
    "Also we can use the sigma matrix to reduce the dimensions of the matrices while retaining most of the information available on the original data. To do that we can select the most weighted values from the sigma matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def rank_k(k):\r\n",
    "    '''\r\n",
    "    Function to reduce the rank to the given level\r\n",
    "    '''\r\n",
    "    U_reduced= np.mat(U[:,:k])\r\n",
    "    Vt_reduced = np.mat(V_t[:k,:])\r\n",
    "    Sigma_reduced = np.eye(k)*sigma[:k]\r\n",
    "    \r\n",
    "    return U_reduced, Sigma_reduced, Vt_reduced\r\n",
    "\r\n",
    "U_reduced, Sigma_reduced, Vt_reduced = rank_k(4)\r\n",
    "M_hat = U_reduced * Sigma_reduced * Vt_reduced\r\n",
    "\r\n",
    "print(M_hat)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4.87147087  3.11444112  0.04893344  2.23870109  1.94083799  1.920736  ]\n",
      " [ 3.49344678  3.45787572  4.19067126  0.94886084  2.61521613  2.82032378]\n",
      " [ 5.22111879  1.8034114   4.91572235  1.58969108  1.09528095  1.14205388]\n",
      " [ 3.25351113  4.77315242  2.90384191 -0.4721446   1.14157873  1.13455568]\n",
      " [ 2.93061675  3.04700483  3.03112668  2.11137004  4.29526848  4.67079756]\n",
      " [ 2.27270952  2.76664391  1.89315701  2.50473044  4.91596291  5.35161957]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here based on the column and row indices,we can identfy the related ratings for a particular item. \r\n",
    "\r\n",
    "Also as we can see, we need to do 3 multiplications to get the M_hat matrix. To avoid that we can only save the decomposed matrices. We will take the squareroot of the Σ matrix and multiply it with each of the other matrices as below to get the decomposed matrices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def rank_k_v2(k):\r\n",
    "    '''\r\n",
    "    Updated version of rank reduction function.\r\n",
    "    '''\r\n",
    "    U_reduced= np.mat(U[:,:k])\r\n",
    "    Vt_reduced = np.mat(V_t[:k,:])\r\n",
    "    Sigma_reduced = np.eye(k)*sigma[:k]\r\n",
    "    Sigma_sqrt = np.sqrt(Sigma_reduced)\r\n",
    "\r\n",
    "    return U_reduced*Sigma_sqrt, Sigma_sqrt*Vt_reduced\r\n",
    "\r\n",
    "U_reduced, Vt_reduced = rank_k_v2(4)\r\n",
    "M_hat = U_reduced * Vt_reduced\r\n",
    "\r\n",
    "print(M_hat)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4.87147087  3.11444112  0.04893344  2.23870109  1.94083799  1.920736  ]\n",
      " [ 3.49344678  3.45787572  4.19067126  0.94886084  2.61521613  2.82032378]\n",
      " [ 5.22111879  1.8034114   4.91572235  1.58969108  1.09528095  1.14205388]\n",
      " [ 3.25351113  4.77315242  2.90384191 -0.4721446   1.14157873  1.13455568]\n",
      " [ 2.93061675  3.04700483  3.03112668  2.11137004  4.29526848  4.67079756]\n",
      " [ 2.27270952  2.76664391  1.89315701  2.50473044  4.91596291  5.35161957]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But the problem in the above is that, we marked the values/ ratings we did not know with zero. It causes the decomposed matrices to generate values close to zero as well. Therefore to avoid this problem we use a technique called `Imputation`.\r\n",
    "\r\n",
    "Basically this means that we can either normalize the each data row such that mean is zero or we can calculate the mean for each user/item and fill the null values with it. Apparently both these techniques are called Imputation methods. By doing this we can get somewhat sensible value for the null values in decomposition."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### __Adding new user to the decomposed matrix__\r\n",
    "\r\n",
    "In SVD method, it is really easy to add new users to the decompsed matrix by nature. Since matrix multiplication works in a certain way, we can directly append user row to the rating (U) matrix.\r\n",
    "\r\n",
    "<pre style='color:yellow'>\r\n",
    "<center>u_reduced = u_original * Vt * Σ^-1</center>\r\n",
    "\r\n",
    "here,\r\n",
    "\r\n",
    "- u_reduced = reduced vector of user ratings.\r\n",
    "- u_original = original ratings given by the new user.\r\n",
    "- Vt = item Matrix transpose from SVD method.\r\n",
    "- Σ^-1 = Inverse of the Σ matrix of SVD.\r\n",
    "\r\n",
    "</pre>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from numpy.linalg import inv\r\n",
    "\r\n",
    "# new user original ratings\r\n",
    "u_original = np.array([4.0, 5.0, 0.0, 3.0, 3.0, 0.0])\r\n",
    "\r\n",
    "# reduced vector\r\n",
    "u_reduced = u_original *Vt_reduced.T* inv(Sigma_reduced)\r\n",
    "\r\n",
    "print(u_reduced)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.45894059  0.23555071  1.9293472  -0.58325078]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### __Adding new item to the decomposed matrix__\r\n",
    "\r\n",
    "Just as we add a new user to the decomposed matrix, we can add new items to the V matrix as well.\r\n",
    "\r\n",
    "<pre style='color:yellow'>\r\n",
    "<center>i_reduced = i_original_t * U * Σ^-1</center>\r\n",
    "\r\n",
    "here,\r\n",
    "\r\n",
    "- i_reduced = the vector in the reduced space to represent the new item.\r\n",
    "- i_original_t = original item ratings transpose vector.\r\n",
    "- U = User Matrix from SVD method.\r\n",
    "- Σ^-1 = Inverse of the Σ matrix of SVD.\r\n",
    "\r\n",
    "</pre>\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> One thing you might wonder is why the heck we are decomposing the matrices filled with mean values and then using the decomposed matrices to estimate the original matrix again. Answer would be, the point of decomposing is reducing the size of original matrix and extracting hidden topics for further processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ## __Recommendations with SVD__\r\n",
    "\r\n",
    "To do recommendations using SVD we can use few methods.\r\n",
    "\r\n",
    "1. Calculate M_hat and use it to find the highest rated item for the user (which they havent consumed). (weird to me as well, apparently it works! :-/ )\r\n",
    "2. Can use the decomposed user matrix to do the collabarative filtering\r\n",
    "3. Using the item similarity on the decomposed item matrix.\r\n",
    "\r\n",
    "Can test the 3 methods to see how it work.\r\n",
    "\r\n",
    "> ## **Problems of SVD**\r\n",
    "\r\n",
    "1. Need to fill out the empty values.\r\n",
    "2. Slow to calculate on large matrices.\r\n",
    "3. Even though we can add new users and items on the go, we need to recalculate the Sigma matrix often to make the decomposition accurate.\r\n",
    "4. Not always explainable.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline Predictors\r\n",
    "\r\n",
    "Practically speaking, Baseline predictors are type of methods that we can use for recommend items to users. \r\n",
    "\r\n",
    "What we do in this technique is that, we measure the inherent bias for items/ users and them use those biases as baseline for our prediction. Intuition behind baseline predictors is that if a item is considered good, its general ratings would be high. Opposite would also be true for the bad items. Also some users would be inherently highly negative/positive. In all these cases we can see some kind of bias towards a item. If we can measure this bias, it can be used as a baseline, which is what we try to do in Baseline predictors.\r\n",
    "\r\n",
    "We use following equation as the predictor.\r\n",
    "\r\n",
    "\r\n",
    "<pre style='color:yellow'>\r\n",
    "<center>r_ui = µ + b_u + b_i</center>\r\n",
    "\r\n",
    "here,\r\n",
    "\r\n",
    "- r_ui = base prediction for item i from user u.\r\n",
    "- µ = average of all ratings\r\n",
    "- b_u = user bias\r\n",
    "- b_i = item bias\r\n",
    "\r\n",
    "</pre>\r\n",
    "\r\n",
    "We can calculate the user bias and item bias terms using existing rating.\r\n",
    "\r\n",
    " 1. Calculate the average of all ratings by taking sum of all elements and dividing by number of non zero elements.\r\n",
    " 2. For the given user select a rating already given and fill the r_ui and µ values. (at this step, we know them)\r\n",
    " 3. Do that for many items. In this way we can get many formulas.\r\n",
    " 4. Solve those equations as a least-square problem to get the bias terms.\r\n",
    "\r\n",
    "\r\n",
    "<center><image src=\"./images/Baseline predictors.jpg\" width=\"200px\" /></center>\r\n",
    "\r\n",
    "Since this calculation is time consuming and comparatively complex, more simpler method is used as well. In this method, we first calculate the bias for each user (b_u), by taking the sum of the difference between user's ratings and the mean. Then we divide that value by number of ratings provided by user. This gives us average difference between the mean and the user's ratings.\r\n",
    "\r\n",
    "<center><image src=\"./images/Baseline User bias.jpg\" width=\"200px\" /></center>\r\n",
    "\r\n",
    "Then after calculating biases for all users we can calculate items biases in the same way as below.\r\n",
    "\r\n",
    "<center><image src=\"./images/Baseline Item bias.jpg\" width=\"200px\" /></center>\r\n",
    "\r\n",
    "Those above values can be used instead of using imputation methods, to make any matrix factorization method to work better. Below is a sample implementation of the related calculations.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "global_mean = M[M>0].mean().mean()  #To calculate the global mean of matrix values\r\n",
    "M_minus_mean = M[M>0] - global_mean  # r_iu - global mean\r\n",
    "user_bias = M_minus_mean.T.mean()\r\n",
    "item_bias = M_minus_mean.apply(lambda r: r - user_bias).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "M_minus_mean"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                mib        st        av         b        ss        lm\n",
       "Sara       2.002778  0.002778       NaN -0.997222 -0.997222 -0.997222\n",
       "Jesper     1.002778  0.002778  1.002778       NaN  0.002778  0.002778\n",
       "Therese    2.002778 -0.997222  2.002778 -0.997222 -1.997222 -1.997222\n",
       "Helle      0.002778  2.002778  0.002778       NaN -1.997222 -1.997222\n",
       "Pietro     0.002778  0.002778  0.002778 -0.997222  1.002778  2.002778\n",
       "Ekaterina -0.997222  0.002778 -0.997222  0.002778  2.002778  2.002778"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mib</th>\n",
       "      <th>st</th>\n",
       "      <th>av</th>\n",
       "      <th>b</th>\n",
       "      <th>ss</th>\n",
       "      <th>lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sara</th>\n",
       "      <td>2.002778</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>-0.997222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesper</th>\n",
       "      <td>1.002778</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>1.002778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Therese</th>\n",
       "      <td>2.002778</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>2.002778</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>-1.997222</td>\n",
       "      <td>-1.997222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helle</th>\n",
       "      <td>0.002778</td>\n",
       "      <td>2.002778</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.997222</td>\n",
       "      <td>-1.997222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pietro</th>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>1.002778</td>\n",
       "      <td>2.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ekaterina</th>\n",
       "      <td>-0.997222</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>-0.997222</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>2.002778</td>\n",
       "      <td>2.002778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "user_bias"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sara        -0.197222\n",
       "Jesper       0.402778\n",
       "Therese     -0.330556\n",
       "Helle       -0.397222\n",
       "Pietro       0.336111\n",
       "Ekaterina    0.336111\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "item_bias"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "mib    0.644444\n",
       "st     0.144444\n",
       "av     0.333333\n",
       "b     -0.783333\n",
       "ss    -0.355556\n",
       "lm    -0.188889\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above values show the bias values for respective items and users. We can see Sara has generally negetive bias and Jesper has positive rating bias. By looking at the original ratings given by those 2 users, we can verify this as well.\r\n",
    "\r\n",
    "> Also one other factor we need to consider here is time. User and item biases may change over time and therefore we need to consider it to make the predictions more accurate. (We can use weighted averages etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funk SVD (Regularized SVD)\r\n",
    "\r\n",
    "This algorithm was developed to fix some of the problems of SVD, mainly to work with sparse matrix (where number of filled values are low). In Funk SVD, we use RMSE along with gradient descent to approach a better matrix factorization.\r\n",
    "\r\n",
    "> ### Root Mean Squared Error\r\n",
    "\r\n",
    "> In the context of user rating and item rating matrix multiplication, we assume that those two create the rating matrix. So RMSE would be a value related to the summation of difference between the actual rating and the values got multiplied from the rating/item matrices.\r\n",
    "\r\n",
    "<center><image src=\"./images/RMSE baseline.jpg\" width=\"200px\" /></center>\r\n",
    "\r\n",
    "> Here `u` means user matrix u row and `i` means item matrix i'th column, which depicts the r_hat in the (u,i) position M_hat matrix.\r\n",
    "\r\n",
    "Then we use gradient descent to minimize the above value.\r\n",
    "\r\n",
    "\r\n",
    "> ### Gradient Descent\r\n",
    "\r\n",
    "> This very well known method in ML. Basically we start in a random position of the output and go through the lowest gradient direction which would minimize the RMSE. Read it in either ML related book or statistic book for more information.\r\n",
    "\r\n",
    "In funk SVD there's no sigma matrix. Instead we directly have 2 matrices. Therefore we first need to figure out number of features we need to keep in the factorized form. This is usually done by running the algorithm for several different number of features and selecting the best value. Also it is worth noting that bias terms are also considered when estimating the factorizations.\r\n",
    "\r\n",
    "When predicting using Funk SVD, we can use the same techniques as the normal SVD without an issue.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "source": [
    "def predict(user, item):\r\n",
    "    avg = all_movies_mean\r\n",
    "    pq = np.dot(item_factors[item], user_factors[user].T)\r\n",
    "    b_ui = avg + user_bias[user] + item_bias[item]\r\n",
    "    prediction = b_ui + pq\r\n",
    "\r\n",
    "    return prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "source": [
    "def stocastic_gradient_descent(index_randomized, ratings):\r\n",
    "    mse = 0\r\n",
    "    \r\n",
    "    for inx in index_randomized:\r\n",
    "        rating_row = ratings[inx]\r\n",
    "        u = inx\r\n",
    "\r\n",
    "        item_randomized = random.sample(range(0, len(rating_row)), len(rating_row))\r\n",
    "        \r\n",
    "        for i in item_randomized:\r\n",
    "            rating = rating_row[i]\r\n",
    "            prediction = predict(u, i)\r\n",
    "            \r\n",
    "            err = (rating - prediction)\r\n",
    "\r\n",
    "            user_bias[u] += b_lr * (err - bias_r * user_bias[u])\r\n",
    "            item_bias[i] += b_lr * (err - bias_r * item_bias[i])\r\n",
    "\r\n",
    "            user_fac = user_factors[u]\r\n",
    "            item_fac = item_factors[i]\r\n",
    "            \r\n",
    "            user_factors[u] +=  lr * (err * item_fac\r\n",
    "                                                - r * user_fac)\r\n",
    "            item_factors[i] +=  lr * (err * user_fac\r\n",
    "                                                - r * item_fac)\r\n",
    "\r\n",
    "            # Calculating RMSE\r\n",
    "            mse += (err) ** 2\r\n",
    "\r\n",
    "    return (mse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "source": [
    "from collections import defaultdict\r\n",
    "\r\n",
    "k = 3  # Number of features to keep in the factorized matrices\r\n",
    "\r\n",
    "movies = ['mib', 'st', 'av', 'b', 'ss', 'lm']\r\n",
    "users = ['Sara', 'Jesper', 'Therese', 'Helle', 'Pietro', 'Ekaterina']\r\n",
    "M = np.array([\r\n",
    "                    [5.0, 3.0, 0.0, 2.0, 2.0, 2.0],\r\n",
    "                    [4.0, 3.0, 4.0, 0.0, 3.0, 3.0],\r\n",
    "                    [5.0, 2.0, 5.0, 2.0, 1.0, 1.0],\r\n",
    "                    [3.0, 5.0, 3.0, 0.0, 1.0, 1.0],\r\n",
    "                    [3.0, 3.0, 3.0, 2.0, 4.0, 5.0],\r\n",
    "                    [2.0, 3.0, 2.0, 3.0, 5.0, 5.0]]\r\n",
    "                )\r\n",
    "\r\n",
    "item_factors = np.random.uniform(size=(len(movies), k))\r\n",
    "user_factors = np.random.normal(size=(len(users), k))\r\n",
    "\r\n",
    "all_movies_mean = M[M>0].mean().mean() \r\n",
    "\r\n",
    "user_bias = defaultdict(lambda: 0)\r\n",
    "item_bias = defaultdict(lambda: 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "source": [
    "import random\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "iterations = 0\r\n",
    "last_err = 0\r\n",
    "iteration_err = sys.maxsize\r\n",
    "finished = False\r\n",
    "\r\n",
    "lr      = 0.05\r\n",
    "b_lr    = 0.01\r\n",
    "r       = 0.001\r\n",
    "bias_r  = 0.001\r\n",
    "\r\n",
    "        \r\n",
    "while not finished:\r\n",
    "\r\n",
    "    index_randomized = random.sample(range(0, len(M)), len(M))\r\n",
    "\r\n",
    "    iteration_err = stocastic_gradient_descent( index_randomized,\r\n",
    "                                                M)\r\n",
    "\r\n",
    "    if(iterations%10==0):\r\n",
    "        print(iteration_err)\r\n",
    "\r\n",
    "    iterations += 1\r\n",
    "    if(iterations >= 100):\r\n",
    "        break\r\n",
    "                        \r\n",
    "    last_err = iteration_err\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100.07094850033116\n",
      "24.16643649902043\n",
      "10.466719350275437\n",
      "5.976312908919969\n",
      "4.744028823950094\n",
      "4.289225986111172\n",
      "4.106737963987126\n",
      "4.037670240023444\n",
      "3.9814951536063043\n",
      "3.949055008435495\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "source": [
    "temp = np.dot(item_factors, user_factors.T).T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "source": [
    "user_bias[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.14336531811425812"
      ]
     },
     "metadata": {},
     "execution_count": 1057
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "source": [
    "item_bias[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4797200154460631"
      ]
     },
     "metadata": {},
     "execution_count": 1058
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "source": [
    "(temp[0] + user_bias[0] + item_bias[0] + all_movies_mean)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.83187567, 3.767716  , 0.2096792 , 3.92966637, 2.33925628,\n",
       "       2.08336076])"
      ]
     },
     "metadata": {},
     "execution_count": 1059
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "source": [
    "M[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5., 3., 0., 2., 2., 2.])"
      ]
     },
     "metadata": {},
     "execution_count": 1060
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore we can observe that, decomposed matrices and biased terms somewhat converge to the original data matrix. But there are several concerns to be addressed. In some scenarios, there may be unexplainable ratings can be observed. This is due to large negative/positive bias terms compared to ratings. We need to fix the solution to address it. Also it is required to fine tune the parameters to get the best results which can be exhausting and time consuming."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}